{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFejFmVeoqBR"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaledDotProductAtt(nn.module):\n",
        "  def __init__(self,scale):\n",
        "    super().__init__()\n",
        "    self.scale = scale\n",
        "    self.softmax = nn.softmax(dim=2)\n",
        "  def forward(self,q,k,v,mask=None):\n",
        "    u = torch.bmm(q,k.transpose(1,2))\n",
        "    u = u / self.scale\n",
        "    if mask is not None:\n",
        "      u = u.masked_fill(mask,-np.inf)\n",
        "    att = self.softmax(u)\n",
        "    output = torch.bmm(att,v)\n",
        "    return att, output\n",
        "\n",
        "def multiHeadAtt(nn.module):\n",
        "  def __init__(self,n_head,d_k_,d_v_,d_k.d_v,d_o):\n",
        "    super().__init__()\n",
        "    self.head = n_head\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "    self.fc_q = nn.Linear(d_q_,self.d_q*n_head)\n",
        "    self.fc_k = nn.Linear(d_k_,self.d_k*n_head)\n",
        "    self.fc_v = nn.Linear(d_v_,self.d_v*n_head)\n",
        "    self.fc_o = nn.Linear(d_v*n_head,self.d_od)\n",
        "    self.attention = scaledDotProductAtt(scale=d_k**0.5)\n",
        "\n",
        "  def forward(self,q,k,v,mask=None):\n",
        "    n_head, d_q, d_k, d_v = self.n_head, self.d_k, self.d_k, self.d_v\n",
        "    batch, n_q, d_q = q.size()\n",
        "    batch, n_k, d_k = k.size()\n",
        "    batch, n_v, d_v = v.size()\n",
        "\n",
        "    q = self.fc_q(q)\n",
        "    k = self.fc_k(k)\n",
        "    v = self.fc_v(v)\n",
        "\n",
        "    q = q.view(batch, n_q, n_head, d_q).permute(2, 0, 1, 3).contiguous().view(-1, n_q, d_q)\n",
        "    k = k.view(batch, n_k, n_head, d_k).permute(2, 0, 1, 3).contiguous().view(-1, n_k, d_k)\n",
        "    v = v.view(batch, n_v, n_head, d_v).permute(2, 0, 1, 3).contiguous().view(-1, n_v, d_v)\n",
        "\n",
        "    if mask is not None:\n",
        "      mask = mask.repeate(n_head,1,1)\n",
        "    attn, output = self.attention(q,k,v,mask)\n",
        "    output = output.view(n_head, batch, n_q, d_v).permute(1, 2, 0, 3).contiguous().view(batch, n_q, -1)\n",
        "    output = self.fc_o(output)\n",
        "    return attn, output\n",
        "\n",
        "class selfAtt(nn.module):\n",
        "  def __init__(self,n_head,d_x,d_k,d_v,d_o):\n",
        "    self.wq = nn.Parameter(torch.Tensor(d_x, d_k))\n",
        "    self.wk = nn.Parameter(torch.Tensor(d_x, d_q))\n",
        "    self.wv = nn.Parameter(torch.Tensor(d_x, d_v))\n",
        "    self.mha = multiHeadAtt(n_head,d_k_=d_k,d_k=d_k,d_v_=d_v,d_v=d_v,d_o=d_o)\n",
        "\n",
        "    self.init_parameters()\n",
        "\n",
        "    def init_parameters(self):\n",
        "        for param in self.parameters():\n",
        "            stdv = 1. / np.power(param.size(-1), 0.5)\n",
        "            param.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "    def forward(self,x,mask=None):\n",
        "      q = torch.matmul(x,self.wq)\n",
        "      k = torch.matmul(x,self.wk)\n",
        "      v = torch.matmul(x,self.wv)\n",
        "\n",
        "      att, output = self.mha(q,k,v,mask)\n",
        "\n",
        "      return att, output\n"
      ],
      "metadata": {
        "id": "fWCLNE4_Y7dl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}